{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2OvlxnFIsNyT"},"source":["# Grupo 1 - Planta --> Convolutional Neural Network (CNN) para procesar imágenes e identificar la clase que corresponde"]},{"cell_type":"markdown","metadata":{"id":"WGdqrNAvsWiF"},"source":["1) Cargar librerías:"]},{"cell_type":"code","metadata":{"id":"gcVLfyLKsaCj","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1666548507487,"user_tz":180,"elapsed":5165,"user":{"displayName":"AndrÉs Daniel Chimuris Gimenez","userId":"02419529842543926973"}},"outputId":"0b7d2c0f-9a67-4cd4-ec4b-87f5667f1059"},"source":["#@title Librerías a usar\n","from tensorflow import keras\n","from tensorflow.keras.layers import Input, Dense, BatchNormalization\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import Reshape, Flatten, Dropout\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.utils import plot_model\n","\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.preprocessing import image\n","import cv2\n","import copy \n","\n","from sklearn.model_selection import train_test_split\n","from keras.utils import np_utils\n","\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import os\n","\n","from PIL import Image\n","\n","from  sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","print(\"Librerías cargadas\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Librerías cargadas\n"]}]},{"cell_type":"markdown","metadata":{"id":"xXate9UMmhDA"},"source":["2) Definir los parámetros:"]},{"cell_type":"code","metadata":{"id":"50H-whvgtEIP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666548507487,"user_tz":180,"elapsed":6,"user":{"displayName":"AndrÉs Daniel Chimuris Gimenez","userId":"02419529842543926973"}},"outputId":"a6cf9f55-540b-473a-a491-5302d42f9bf3"},"source":["#@title Selección de Parámetros \n","\n","# \n","#@markdown ### Parámetros de imágenes:\n","imagen_largo_ancho =  300#@param {type:\"integer\"}\n","imagen_color = True #@param {type:\"boolean\"}\n","imagen_usar_generadas_data_augmentation = False #@param {type:\"boolean\"}\n","\n","#@markdown ### Parámetros de la red:\n","cnn_tamaño_kernel =  2#@param {type:\"integer\"}\n","cnn_tamaño_pooling = 2 #@param {type:\"integer\"}\n","cnn_cantidad_capas_ocultas =  4#@param {type:\"integer\"}\n","cnn_agregar_capa_BatchNormalization = True #@param {type:\"boolean\"}\n","cnn_agregar_capa_DropOut = False #@param {type:\"boolean\"}\n","cnn_cantidad_neuronas_Dense =  \"8\" #@param {type:\"string\"}\n","cnn_tipo_capa_salida = 'softmax-MultiClase' #@param [\"lineal-Numero\", \"softmax-MultiClase\"]\n","cnn_cant_epocas_entrenamiento =  100#@param {type:\"integer\"}\n","\n","## aplicación de los parámetros elegidos\n","\n","# tamaño de las imágenes\n","if imagen_largo_ancho<=10:\n","  imagen_largo_ancho = 10\n","IMAGE_SHAPE = (imagen_largo_ancho, imagen_largo_ancho, (3 if imagen_color else 1))\n","\n","# indica si se usan las imágenes generadas por data augmentation\n","usarDA = imagen_usar_generadas_data_augmentation\n","\n","# tamaño de los kernels y pooling (para simplificar son todas iguales)\n","if cnn_tamaño_kernel<1:\n","  cnn_tamaño_kernel = 1\n","cnn_kernel_shape = (cnn_tamaño_kernel, cnn_tamaño_kernel)\n","if cnn_tamaño_pooling<1:\n","  cnn_tamaño_pooling=1\n","cnn_pooling_shape = (cnn_tamaño_pooling, cnn_tamaño_pooling)\n","\n","# indica la configuración para la parte Encoder \n","#   (cada elemento de las listas son la configuración de las capas Conv)\n","#cnn_filters = [ 128, 64, 32, 16, 8 ] \n","if cnn_cantidad_capas_ocultas<1:\n","  cnn_cantidad_capas_ocultas = 1\n","cnn_filters = []\n","for i in range(cnn_cantidad_capas_ocultas, 0, -1):\n","  cnn_filters.append( 2**(i+2) )\n","last_conv_layer_name = None\n","last_conv_layer_shape = None\n","\n","# define tamaño de capa densa previa a la de salida\n","num_Dense_out = []\n","for val in cnn_cantidad_neuronas_Dense.split(\",\"):\n","  num_Dense_out.append( int(val) )\n","if len(num_Dense_out)<1:\n","   num_Dense_out.append( 8 )\n","#num_Dense_out = (8 if cnn_cantidad_neuronas_Dense<1 else cnn_cantidad_neuronas_Dense)\n","\n","# define si el tipo de capa de salida es softmax( True )  o lineal ( False )\n","# esto implica también cambiar cómo se codifican los valores de las clases a usar\n","tipo_output_softMax = (cnn_tipo_capa_salida[:7] == 'softmax')\n","\n","# cantidad de épocas del entrenamiento\n","cantEpocas = (100 if cnn_cant_epocas_entrenamiento<1 else cnn_cant_epocas_entrenamiento)\n","\n","print(\"Configuración de la CNN definida: \")\n","print (\"     -Tamaño Imagen: \", IMAGE_SHAPE)\n","print (\"     -Kernels + Pooling: [\", cnn_kernel_shape, \"+\", cnn_pooling_shape, \"] \")\n","print (\"     -Capas: [ \", IMAGE_SHAPE, \", Conv\", cnn_filters, (\", BatchNormalization\" if  cnn_agregar_capa_BatchNormalization else \"\" ), \", FLATTEN, Dense[\", num_Dense_out, \"], \", (\"Softmax\" if tipo_output_softMax else \"Dense[1] \"))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Configuración de la CNN definida: \n","     -Tamaño Imagen:  (300, 300, 3)\n","     -Kernels + Pooling: [ (2, 2) + (2, 2) ] \n","     -Capas: [  (300, 300, 3) , Conv [64, 32, 16, 8] , BatchNormalization , FLATTEN, Dense[ [8] ],  Softmax\n"]}]},{"cell_type":"markdown","metadata":{"id":"-Rm33ZPNnBpE"},"source":["3) Montar el Drive:"]},{"cell_type":"code","metadata":{"id":"ysaIl300nDud","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1666548535696,"user_tz":180,"elapsed":28213,"user":{"displayName":"AndrÉs Daniel Chimuris Gimenez","userId":"02419529842543926973"}},"outputId":"02efc0d7-9078-4900-e659-32e585350de9"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","# directorio local en Google Drive\n","path = 'gdrive/My Drive/IA/demoML/imagenes/PLANTAS_TP3/Fotos_TP3_Zoom_Out/300x300' #@param {type:\"string\"}\n","path_entrenamiento = '/train'  #@param {type:\"string\"}\n","path_prueba = '/test'  #@param {type:\"string\"}\n","\n","imagPath_train = path + path_entrenamiento\n","imagPath_test = path + path_prueba"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"markdown","metadata":{"id":"sDkaUtZ6nG8l"},"source":["4) Cargar imágenes para entrenar el modelo:"]},{"cell_type":"code","metadata":{"id":"uYz8mV4SnJ4O","cellView":"form"},"source":["#@title Cargar imágenes\n","# define función para cargar las imágenes\n","def cargarImagenes(imagPath):\n","  classes_ori = [] \n","  images_ori = []\n","  esDA_ori = []\n","\n","  all_dirs = os.listdir( imagPath )\n","  for each_dir in all_dirs:\n","\n","      auxiPath = imagPath + '/' + each_dir \n","      imagFN  = os.listdir( auxiPath )\n","      for each_imagFN in imagFN:\n","\n","            esImagDA = (each_imagFN[:2] == 'da')\n","            \n","            if usarDA or (not esImagDA): \n","                \n","                # abre la imagen\n","                imag = Image.open(auxiPath + \"/\" + each_imagFN)\n","                \n","                # ajusta el tamaño\n","                if IMAGE_SHAPE[2]==1:              \n","                  tipoImage = 'L'\n","                else:                \n","                  tipoImage = 'RGB'\n","                imag = imag.convert(tipoImage)\n","                imag = imag.resize((IMAGE_SHAPE[0], IMAGE_SHAPE[1]), Image.ANTIALIAS)          \n","                \n","                # transforma a un vector de nros\n","                arImag = np.array(imag)\n","                \n","                # agrega a los vectores\n","                classes_ori.append( each_dir )\n","                images_ori.append( arImag )\n","                esDA_ori.append( esImagDA )\n","\n","  return classes_ori, images_ori, esDA_ori, tipoImage\n","\n","# carga las imagenes de entrenamiento\n","classes_train, images_train, esDAimag_train, tipoImage_train = cargarImagenes(imagPath_train)\n","print(\"> Para Entrenamiento: \")\n","print(\"- Clases cargadas: \", len(classes_train))\n","print(\"- Imágenes cargadas: \", len(classes_train))\n","\n","if len(classes_train)>0:\n","  print(\"- Ejemplo \", classes_train[0], \" \", images_train[0].shape, \": \")\n","  display( Image.fromarray(images_train[0], tipoImage_train) )\n","\n","# carga las imagenes de prueba\n","classes_test, images_test, esDAimag_test, tipoImage_test = cargarImagenes(imagPath_test)\n","print(\"\\n\\n> Para Prueba: \")\n","print(\"- Clases cargadas: \", len(classes_test))\n","print(\"- Imágenes cargadas: \", len(images_test))\n","\n","if len(classes_test)>0:\n","  print(\"- Ejemplo \", classes_test[0], \" \", images_test[0].shape, \": \")\n","  display( Image.fromarray(images_test[0], tipoImage_test) )"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Ajustar imágenes para reducir el fondo (opcional)\n","\n","accion_realizar = \"Blur Fondo\" #@param [\"-\", \"Blur Fondo\", \"Eliminar Fondo y pasar a Negro\", \"Eliminar Fondo y pasar a Blanco\"]\n","\n","def cambiarColorNegro(img, nuevoColor=[255, 255, 255]):\n","    black_pixels = np.where(\n","        (img[:, :, 0] == 0) & \n","        (img[:, :, 1] == 0) & \n","        (img[:, :, 2] == 0)\n","    )\n","    img[black_pixels] = nuevoColor\n","    return img\n","\n","def blurFondoImagen(im):\n","  # Convert to the HSV color space\n","  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","  # aplica filtro Otsu threshold para obtener máscara\n","  ret, maskthresh = cv2.threshold(hsv[:,:,0], 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n","  # Genera la máscara negada\n","  maskthresh = 255 - maskthresh\n","  # We need a to copy the mask 3 times to fit the frames\n","  maskthresh = np.repeat(maskthresh[:, :, np.newaxis], 3, axis=2)\n","  #  Create a blurred frame using Gaussian blur\n","  blurred_frame = cv2.GaussianBlur(im, (25, 25), 0)\n","  # Combine the original with the blurred frame based on mask\n","  return np.where(maskthresh == (255, 255, 255), im, blurred_frame)\n","\n","def reducirFondoImagen(im):\n","  # aplica filtro Hue\n","  imhsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n","  # aplica filtro Otsu threshold para obtener máscara\n","  ret, maskthresh = cv2.threshold(imhsv[:,:,0], 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n","  # Genera la máscara negada\n","  maskthresh = 255 - maskthresh\n","  # aplica la máscara sobre la imagen\n","  imgfin = cv2.bitwise_and(im, im, mask = maskthresh)\n","  return imgfin\n","\n","def procesarImgRedFondo(imgList):\n","  nList = []\n","  for im in imgList:\n","    if accion_realizar == \"Blur Fondo\":\n","      # hacer blur del fondo\n","      imn = blurFondoImagen(im)\n","    elif accion_realizar == \"Eliminar Fondo y pasar a Negro\":\n","      # eliminar fondo y dejar negro\n","      imn = reducirFondoImagen(im)\n","    elif accion_realizar == \"Eliminar Fondo y pasar a Blanco\":\n","        # cambia fondo negro a casi negro \n","        # (para que no cambié después) \n","        imn = cambiarColorNegro(im, [0, 0, 1])\n","        # eliminar fondo\n","        imn = reducirFondoImagen(imn)\n","        # cambiar fondo a blanco\n","        imn = cambiarColorNegro(imn, [255, 255, 255])\n","    else:\n","      print(\"Acción no definida!\")\n","      break\n","    nList.append( imn )\n","  return nList\n","\n","\n","# degermina si hace algo o no\n","if accion_realizar != \"-\":\n","  # aplica filtros para intentar reducir el fondo de la imagen\n","  # cambiando las imágenes disponibles\n","  images_train = procesarImgRedFondo(images_train)\n","  images_test = procesarImgRedFondo(images_test)\n","\n","  if len(classes_train)>0:\n","    print(\"- Ejemplo Entrenamiento con fondo reducido \", classes_train[0], \" \", images_train[0].shape, \": \")\n","    display( Image.fromarray(images_train[0], tipoImage_train) )\n","\n","  if len(classes_test)>0:\n","    print(\"- Ejemplo Prueba con fondo reducido \", classes_test[0], \" \", images_test[0].shape, \": \")\n","    display( Image.fromarray(images_test[0], tipoImage_test) )"],"metadata":{"cellView":"form","id":"g-Z21zWxFG_Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPPvnkjTnTQN","cellView":"form"},"source":["#@title Preparar imágenes (formato)\n","\n","# define función auxiliar para mostrar imágenes preparadas\n","def plot_image(imag):\n","  if IMAGE_SHAPE[2]==1:\n","    plt.imshow((imag*255).reshape(IMAGE_SHAPE[0], IMAGE_SHAPE[1]).astype(np.uint8))\n","    plt.gray()\n","  else:\n","    plt.imshow((imag*255).reshape(IMAGE_SHAPE).astype(np.uint8))\n","  plt.axis(\"off\")  \n","\n","# define función auxiliar para preparar la lista de imágenes a procesar\n","def prepare_imageList(imagList):    \n","  auxiAr = np.array(imagList).astype('float32') / 255.\n","  auxiAr = auxiAr.reshape((len(auxiAr), IMAGE_SHAPE[0], IMAGE_SHAPE[1], IMAGE_SHAPE[2]))  \n","  return auxiAr\n","  \n","# define función auxiliar para preparar lista de clases \n","def prepare_clasesList(classesList, dictMapeo=None):\n","  if dictMapeo==None:\n","    # genera diccionario de mapeo\n","    auxDict = list(set(classesList))\n","    dictMapeo = dict( zip( auxDict, range(len(auxDict)) ) )\n","  # realiza el mapeo\n","  y = []\n","  for cl in classesList:\n","      y.append( dictMapeo[cl] )\n","  # convierte valores numéricos a columnas de vakores binarios (i.e. one hot encoded)\n","  dummy_y = np_utils.to_categorical(y)\n","  # devuelve\n","  return np.array(y), np.array(dummy_y), dictMapeo\n","\n","# define vector auxiliar de datos de entrada para usar en el entrenamiento y prueba\n","x_train = prepare_imageList(images_train)\n","x_test = prepare_imageList(images_test)\n","\n","# define vector auxiliar de datos de salida para usar en el entrenamiento y prueba\n","# también usa esta información para determinar la cantida de neuronas de salida\n","y_train, y_trainEnc, dictMapeo = prepare_clasesList(classes_train)\n","y_test, y_testEnc,_ = prepare_clasesList(classes_test, dictMapeo)\n","\n","# genera diccionario auxiliar para poder convertir de ID de clase a nombre de clase\n","clases_map = [ x for x,y in dictMapeo.items() ]\n","\n","print(\"> Para Entrenamiento: \")\n","print(\" - x_train (cant ejemplos, datos entrada): \", x_train.shape)\n","if tipo_output_softMax:\n","  print(\" - y_trainEnc (cant): \", len(y_trainEnc))\n","else:\n","  print(\" - y_train (cant): \", len(y_train))\n","print(\"\\n\\n> Para Prueba: \")\n","print(\" - x_test (cant ejemplos, datos entrada): \", x_test.shape)\n","if tipo_output_softMax:\n","  print(\" - y_testEnc (cant): \", len(y_testEnc))\n","else:\n","  print(\" - y_test (cant): \", len(y_test))\n","print(\"\\n\\n> Para Ambos: \")\n","print(\" - dictMapeo: \", dictMapeo)\n","print(\" - clases_map: \", clases_map)\n","if len(y_train)>0:\n","  print(\"\\n - Imagen reconstruida de \", clases_map[y_train[0]],  \"(\", y_train[0], \" / \", y_trainEnc[0], \")\")\n","  plot_image(x_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvooB4Gws7ua"},"source":["5) Establecer el modelo para la CNN:"]},{"cell_type":"code","metadata":{"id":"k_ehBHOatp4H","cellView":"form"},"source":["#@title Establecer modelo\n","\n","# define la arquitectura de capas de la CNN\n","# teniendo en cuenta la definición dada anteriomente\n","input_img_Lay = Input(shape=IMAGE_SHAPE, name='input_img') # capa de entrada\n","eachLay = input_img_Lay\n","auxName = 'conv_'\n","for i in range(len(cnn_filters)):  \n","\n","    # define el nombre de la capa oculta\n","    auxlayerName = 'conv_'+str(i+1)\n","\n","    # agrega las capas ocultas de tipo Conv2D + MaxPooling \n","    eachLay = Conv2D(cnn_filters[i], cnn_kernel_shape, activation='relu', padding='same', name='c_'+auxlayerName)(eachLay) \n","    # determina nombre y shape de la capa conv2D\n","    last_conv_layer_name = 'c_'+auxlayerName\n","    last_conv_layer_shape = (eachLay.shape[1], eachLay.shape[2])\n","    ##print(last_conv_layer_name, last_conv_layer_shape, eachLay.shape)\n","    eachLay = MaxPooling2D(cnn_pooling_shape, padding='same', name='p_'+auxlayerName)(eachLay)\n","\n","if cnn_agregar_capa_BatchNormalization:\n","  # nota: para convNet no se usa DropOut, conviene usar BatchNormalization\n","  eachLay = BatchNormalization(name='bn_dense')(eachLay)\n","\n","#  agrega capa Flatten y Dense \n","eachLay = Flatten(name='flat_'+auxlayerName)(eachLay)\n","\n","# agrega capa DropOut\n","if cnn_agregar_capa_DropOut:\n","  eachLay = Dropout(0.2)(eachLay)\n","\n","iDense = 1\n","for cantDense in num_Dense_out:\n","  eachLay = Dense(cantDense, activation='relu', name='d_'+auxlayerName+str(iDense))(eachLay)\n","  iDense = iDense + 1\n","\n","# agrega capa de salida\n","if tipo_output_softMax:\n","    # se genera una capa softmax\n","    output_img_Lay = Dense(units = len(dictMapeo), activation='softmax', name='output')(eachLay) # capa de salida\n","else:\n","    # se genera una capa lineal con una salida numérica\n","    output_img_Lay = Dense(1, activation=None, name='output')(eachLay) # capa de salida\n","\n","# genera el modelo ConvNet\n","model = Model(input_img_Lay, output_img_Lay, name='ConvNet')\n","if tipo_output_softMax:\n","    # utiliza un loss de multiple clases\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","else:\n","    # utiliza un loss de valor numérico\n","    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","\n","print(\"Modelo CNN creado con \", len(model.layers), \" capas:\")\n","model.summary()\n","print(\"\\n\")\n","plot_model(model, show_layer_names=True, show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBtXyyXCtjDc"},"source":["6) Entrenar el modelo de la RNA:"]},{"cell_type":"code","metadata":{"id":"21pQvmtCtn-T","cellView":"form"},"source":["#@title Entrenar\n","\n","# separa al azar usando muestreo al azar del 10%\n","# para tomar algunos como datos de validación\n","x_t, x_v, y_t, y_v = train_test_split(x_train, \n","                                       (y_trainEnc if tipo_output_softMax else y_train), \n","                                       test_size=0.05)\n","\n","print(\"\\n> De los \", len(x_train), \"ejemplos de entrenamiento: \")\n","print(\"            se usan \", len(x_t), \"ejemplos para entrenar \")\n","print(\"            y \", len(x_v), \"ejemplos para validar.\")\n","\n","print(\"\\n\\n>Comienza el Entrenamiento:\")\n","\n","# lleva a cabo el entrenamiento\n","history = model.fit(x_t, y_t,\n","          epochs = cantEpocas, \n","          validation_data=(x_v, y_v,),\n","          callbacks=[] ) \n","\n","print(\"\\n>Entrenamiento Finalizado.\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MxcR9WBKx4XA","cellView":"form"},"source":["#@title Mostrar Gráficos del Entrenamiento\n","plt.figure(figsize=(15,8)) \n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Gráfico del Error del Entrenamiento')\n","plt.ylabel('')\n","plt.xlabel('epoch')\n","plt.legend(['entrenamiento', 'validación'], loc='upper left')\n","plt.show()\n","\n","plt.figure(figsize=(15,8)) \n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Gráfico de la Exactitud del Entrenamiento')\n","plt.ylabel('')\n","plt.xlabel('epoch')\n","plt.legend(['entrenamiento', 'validación'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzmLDXuUHkdf","cellView":"form"},"source":["#@title Probar red entrenada con datos de entrenamiento\n","mostrar_detalle_imagenes_entrenamiento = False #@param {type:\"boolean\"}\n","\n","mostrar_HeatMap_de_GradCAM = False #@param {type:\"boolean\"}\n","  # explicación y fuente de GradCAM en:\n","  #   https://medium.com/analytics-vidhya/visualizing-activation-heatmaps-using-tensorflow-5bdba018f759\n","\n","def prepareGradCAM(model, last_conv_layer_name):\n","  # genera sub-modelo desde capa de entrada a última capa convultional\n","  last_conv_layer = model.get_layer(last_conv_layer_name)\n","  subModel = tf.keras.models.Model([model.inputs], [model.output, last_conv_layer.output])                   \n","  return subModel\n","\n","def aplicarGradCAM(subModel, last_conv_layer_shape, imgOrig, intensity=0.5, res=250):\n","  # prepara la imagen a procesar\n","  img = copy.deepcopy(imgOrig)\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)  \n","  # genera el heatmap\n","  with tf.GradientTape() as tape:\n","    model_out, last_conv_layer = subModel(x)\n","    class_out = model_out[:, np.argmax(model_out[0])]\n","    grads = tape.gradient(class_out, last_conv_layer)\n","    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n","  heatmap = tf.reduce_mean(tf.multiply(pooled_grads, last_conv_layer), axis=-1)\n","  heatmap = np.maximum(heatmap, 0)\n","  heatmap /= np.max(heatmap)\n","  heatmap = heatmap.reshape(last_conv_layer_shape) # (8, 8))\n","  # aplica el heatmap\n","  imgHM = copy.deepcopy(imgOrig)\n","  heatmap = cv2.resize(heatmap, (imgHM.shape[1], imgHM.shape[0]))\n","  heatmap = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n","  imgHM = heatmap * intensity + imgHM\n","  # devuelve imagen con HeatMap\n","  return imgHM\n","\n","# función auxiliar para probar el modelo entrenado en detalle\n","def probarModelo(x, y, esDAimag, clases_map, mostrarImagenes=False, mostrarGradCAM=False):\n","\n","    if mostrarGradCAM:\n","      # genera submodelo\n","      subModel = prepareGradCAM(model, last_conv_layer_name)\n","      # si muestra GradCAM también muestra imagen original\n","      mostrarImagenes = True\n","\n","    # procesa las imágenes de prueba con el modelo \n","    predClass = model.predict(x)\n","\n","    if mostrarImagenes:\n","      print(\"\\n>Resultados: \")\n","\n","    # muestra los resultados con las imágenes \n","    umbralClas = 0.5\n","    classPreds = []\n","    classReal = []\n","    for i in range(len(x)):\n","\n","        # asigna el nombre de la clase real\n","        clReal = clases_map[ y[i] ] \n","\n","        # determina la clase predecida\n","        if tipo_output_softMax:\n","            ## determina clase predecida de acuerdo a la que tiene mayor valor\n","            idclPred = int( np.argmax(predClass[i], axis=0) )\n","            idclPredRnd = idclPred\n","        else:\n","            ## determina clase predecida de acuerdo al umbral de clasificación\n","            idclPred = predClass[i][0]       \n","            idclPredRnd = int(idclPred)\n","            if (idclPred - idclPredRnd)>0.5 and (idclPredRnd+1)<len(clases_map):\n","                    idclPredRnd = idclPredRnd + 1\n","\n","        # asigna el nombre de la clase predecida\n","        if idclPredRnd<0 or idclPredRnd>=len(clases_map):\n","            clPred = \"CLASE \" + str(idclPredRnd) + \" INVÁLIDA!\"\n","        else:      \n","            clPred = clases_map[ idclPredRnd ]\n","\n","        # agrega a vevtores auxiliares\n","        classReal.append( clReal )\n","        classPreds.append( clPred )\n","\n","        if mostrarImagenes:\n","\n","          # sólo muestra las imágenes no generadas por DA\n","          strTitulo = 'Real: ' + clReal + ' / RNA: ' \n","          strTitulo = strTitulo + clPred + ' (' + str( idclPred ) +')'    \n","\n","          # muestra comparación con la imagen\n","          fig = plt.figure()\n","          fig.suptitle( strTitulo )\n","          ax1 = fig.add_subplot(121)\n","          plot_image( x[i] )\n","          # muestra resultado aplicar GRADCAM\n","          if mostrarGradCAM:            \n","            imgGradCAM = aplicarGradCAM(subModel, last_conv_layer_shape, x[i]) \n","            ax1 = fig.add_subplot(122)\n","            plot_image(  imgGradCAM)  \n","          plt.tight_layout()\n","          fig = plt.gcf()\n","          plt.show()\n","          plt.close(fig)          \n","\n","    # muestra reporte de clasificación\n","    print(\"\\n Reporte de Clasificación: \")\n","    print(classification_report(classReal, classPreds))\n","\n","    # muestra matriz de confusion\n","    print('\\nMatriz de Confusión ( real / modelo ): ')\n","    cm = confusion_matrix(classReal, classPreds, labels=clases_map)\n","    cmtx = pd.DataFrame(\n","        cm, \n","        index=['r:{:}'.format(x) for x in clases_map], \n","        columns=['m:{:}'.format(x) for x in clases_map]\n","      )\n","    print(cmtx)\n","    print(\"\\n\")\n","\n","# prueba con los datos de prueba\n","print(\"*** Resultados con datos de Entrenamiento: \")\n","probarModelo(x_train, y_train, esDAimag_train, clases_map, mostrar_detalle_imagenes_entrenamiento, mostrar_HeatMap_de_GradCAM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yh-6p2xDtrUU"},"source":["7) Evaluar el modelo de la RNA entrenado usando las imágenes de prueba:"]},{"cell_type":"code","metadata":{"id":"A15K-9TRtq7U","cellView":"form"},"source":["#@title Probar red entrenada con datos de prueba\n","mostrar_detalle_imagenes_prueba = False #@param {type:\"boolean\"}\n","mostrar_HeatMap_de_GradCAM_prueba = False #@param {type:\"boolean\"}\n","\n"," # evalua al modelo entrenado\n","resEval = model.evaluate(x_test, (y_testEnc if tipo_output_softMax else y_test),)\n","print(\"\\n>Evaluación del Modelo: \")\n","print(\"    - Error: \", resEval[0])\n","print(\"    - Exactitud: \", resEval[1]*100)\n","print(\"\\n\")\n","\n","# prueba con los datos de entrenamiento\n","print(\"\\n\\n*** Resultados con datos de Prueba: \")\n","probarModelo(x_test, y_test, esDAimag_test, clases_map, mostrar_detalle_imagenes_prueba, mostrar_HeatMap_de_GradCAM_prueba)"],"execution_count":null,"outputs":[]}]}